{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Model",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wifzT3LIDtMr"
      },
      "source": [
        "Install this package to load the dataset directly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFg4KDvAJJic"
      },
      "source": [
        "pip install -q git+https://github.com/eaedk/testing-zindi-package.git"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6ZNakhnwA5_"
      },
      "source": [
        "Install this package for hparam tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYJhUoBpwDHe"
      },
      "source": [
        "pip install -q -U keras-tuner"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5gfIC2sD88Y"
      },
      "source": [
        "Import everything that we need"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzrPGGHOOxHY"
      },
      "source": [
        "from zindi.user import Zindian\n",
        "from tensorflow.keras.applications import ResNet50V2\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imutils import paths\n",
        "from tabulate import tabulate\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import mimetypes\n",
        "import argparse\n",
        "import imutils\n",
        "import pickle\n",
        "import cv2\n",
        "import os\n",
        "import statistics\n",
        "import keras_tuner as kt\n",
        "import tensorflow as tf"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6YFubVvECHX"
      },
      "source": [
        "Sign in to Zindi account"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kGT-KzMO8ti",
        "outputId": "f2d96bf4-0e43-414e-ae57-6144492bad04"
      },
      "source": [
        "zindi_username = \"Marcus\"\n",
        "zindi_account = Zindian(username = zindi_username)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your password\n",
            ">> 路路路路路路路路路路\n",
            "\n",
            "[  ] 攫 Welcome Marcus 攫\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwvmW9ITEO96"
      },
      "source": [
        "Select the makerere-passion-fruit-disease-detection-challenge and show the details"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KyAtwM93PiRv",
        "outputId": "46dd9c1b-fec9-487a-c378-d14799c1d1e2"
      },
      "source": [
        "zindi_account.select_a_challenge()\n",
        "zindi_account.which_challenge"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__________________________________________________________________________________________________________________________________\n",
            "|     |              |                  |                    |          \n",
            "|index|  challenge   |     problem      |       reward       |    id    \n",
            "|     |              |                  |                    |          \n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "|  0  |Public Compet |  Classification  |     $5 000 USD     | kenyan-sign-language-classification-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "|  1  | Public Hack  |  Classification  |      $300 USD      | kenyan-sign-language-classification-hackathon...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "|  2  |Public Compet |  Classification  |     $7 500 USD     | bloodsai-blood-spectroscopy-classification-challen...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "|  3  |Public Compet |Automatic Speech R|     $3 000 USD     | mozilla-luganda-automatic-speech-recognition...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "|  4  |Public Compet |    Prediction    |     $3 000 USD     | zindi-user-behaviour-birthday-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "|  5  |Public Compet |  Classification  |     Knowledge      | road-segment-identification...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "|  6  |Public Compet |    Prediction    |     $1 000 USD     | expresso-churn-prediction...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "|  7  |Public Compet |  Classification  |     $1 000 USD     | makerere-passion-fruit-disease-detection-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "|  8  |Public Compet |  Classification  |    2000 Points     | gender-based-violence-tweet-classification-challen...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "|  9  |Public Compet |     Forecast     |     $8 800USD      | radiant-earth-spot-the-crop-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 10  |Public Compet |     Forecast     |     $8 800 USD     | radiant-earth-spot-the-crop-xl-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 11  |Public Compet |    Prediction    |     Knowledge      | local-ocean-conservation-sea-turtle-face-detection...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 12  |Public Compet |  Classification  |     Knowledge      | zindiweekendz-learning-to-vaccinate-or-not-to-vacc...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 13  |Public Compet |  Classification  |     Knowledge      | swahili-news-classification...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 14  |Public Compet |    Prediction    |     Knowledge      | zindiweekendz-learning-urban-air-pollution-challen...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 15  |Public Compet |  Classification  |     Knowledge      | zindiweekendz-learning-covid-19-tweet-classificati...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 16  |Public Compet |    Prediction    |     Knowledge      | zindiweekendz-learning-south-african-covid-19-vuln...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 17  |Public Compet | Computer Vision  |     Knowledge      | zindiweekendz-learning-spot-the-mask-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 18  |Public Compet |    Prediction    |     Knowledge      | tanzania-tourism-prediction...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 19  |Public Compet |    Prediction    |     Knowledge      | data-science-nigeria-2019-challenge-1-insurance-pr...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 20  |Public Compet |  Classification  |     Knowledge      | ai-hack-tunisia-1-computer-vision-challenge-1...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 21  |Public Compet |    Prediction    |     Knowledge      | ai-tunisia-hack-5-predictive-analytics-challenge-2...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 22  |Public Compet |  Classification  |     Knowledge      | ai-hack-tunisia-6-predictive-analytics-challenge-3...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 23  |Public Compet | Computer Vision  |     Knowledge      | ai-hack-tunisia-2-computer-vision-challenge-2...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 24  |Public Compet |  Classification  |     Knowledge      | ai-hack-tunisia-4-predictive-analytics-challenge-1...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 25  |Public Compet | Computer Vision  |     Knowledge      | miia-pothole-image-classification-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 26  |Public Compet | Computer Vision  |     Knowledge      | sbtic-animal-classification...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 27  |Public Compet |    Prediction    |     Knowledge      | financial-inclusion-in-africa...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 28  |Public Compet | Computer Vision  |     Knowledge      | cmu-africa-data-science-club-challenge-1-computer-...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 29  |Public Compet |    Prediction    |     Knowledge      | data-science-nigeria-challenge-2-recommendation-en...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 30  |Public Compet |    Prediction    |     Knowledge      | data-science-nigeria-challenge-1-loan-default-pred...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 31  | Public Hack  |                  |        $800        | coral-classification-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 32  | Public Hack  | Computer Vision  |      $300 USD      | marine-invertebrates-classification-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 33  | Public Hack  |  Classification  |      $300 USD      | road-segment-identification-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 34  | Public Hack  |  Classification  |     $1 000 USD     | amld...   \n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 35  | Public Hack  |  Classification  |      $300 USD      | gender-based-violence-tweet-classification...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 36  | Public Hack  |     Forecast     |        $300        | radiant-earth-spot-the-crop-hackathon...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 37  | Public Hack  |                  |      $700 USD      | silicon-valley-21st-century-education-hackathon...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 38  |Public Compet |    Prediction    |     $1 000 USD     | cryptocurrency-closing-price-prediction...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 39  |Public Compet |    Prediction    |     $5 000 USD     | sfc-paygo-solar-credit-repayment-competition...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 40  | Public Hack  |    Prediction    |      $300 USD      | sfc-paygo-solar-credit-repayment-hackathon...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 41  |Public Compet |  Visualisation   |     $5 000 USD     | afd-solutions-for-gender-based-violence-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 42  |Public Compet |    Prediction    |Scholarship worth $3| ix-mobile-banking-prediction-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 43  |Public Compet |  Visualisation   |     $1 500 USD     | deepfake-africa-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 44  |Public Compet |    Prediction    |     $1 000 USD     | airqo-low-cost-air-quality-monitor-calibration-cha...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 45  | Public Hack  |    Prediction    |      $250 USD      | airqo-air-sensor-calibration-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 46  |Public Compet |    Prediction    |    2000 Points     | economic-well-being-prediction-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 47  | Public Hack  |    Prediction    |     $1,000 USD     | womens-hackathon...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 48  |Public Compet |    Prediction    |    $10 000 USD     | lacuna-correct-field-detection-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 49  |Public Compet |    Prediction    |     $1 000 USD     | autoinland-vehicle-insurance-claim-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 50  |Public Compet |    Collection    |     $1 000 USD     | afd-gender-based-violence-dataset-collection-chall...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 51  |Public Compet |Automatic Speech R|     $2 000 USD     | ai4d-baamtu-datamation-automatic-speech-recognitio...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 52  |Public Compet |  Classification  |     $2 000 USD     | ai4d-malawi-news-classification-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 53  |Public Compet |   Translation    |     $2 000 USD     | ai4d-takwimu-lab-machine-translation-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 54  |Public Compet |   Translation    |     $2 000 USD     | ai4d-yoruba-machine-translation-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 55  |Public Compet |  Classification  |     $2 000 USD     | ai4d-icompass-social-media-sentiment-analysis-for-...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 56  |Public Compet |  Classification  |   Job Interview    | instadeep-enzyme-classification-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 57  |Public Compet |    Prediction    |     $3 000 USD     | cgiar-crop-yield-prediction-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 58  |Public Compet |    Prediction    |     $6 000 USD     | uber-nairobi-ambulance-perambulation-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 59  |Public Compet |  Classification  |     $7 000 USD     | giz-nlp-agricultural-keyword-spotter...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 60  |Public Compet | Computer Vision  |     $3 000 USD     | cgiar-wheat-growth-stage-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 61  |Public Compet |  Classification  | 2000 Zindi Points  | runmila-ai-institute-minohealth-ai-labs-tuberculos...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 62  |Public Compet |    Prediction    |    $25 000 USD     | usaids-intelligent-forecasting-challenge-model-fut...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 63  |Public Compet |    Prediction    |     $5 000 USD     | zimnat-insurance-recommendation-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 64  |Public Compet |Reinforcement Lear| 3000 Zindi Points  | indaba-grand-challenge-curing-leishmaniasis...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 65  | Public Hack  |    Prediction    |     Knowledge      | fighting-fire-with-data-hackathon...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 66  | Public Hack  |  Classification  |    225,000 Tsh     | swahili-news-classification-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 67  | Public Hack  | Computer Vision  |     $1,000 USD     | sansa-informal-settlements-in-south-africa...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 68  |Public Compet |    Collection    |     $6 000 USD     | ai4d-african-language-dataset-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 69  | Public Hack  |    Prediction    |      $300 USD      | the-zimnat-insurance-assurance-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 70  |Public Compet |    Prediction    |     $3 000 USD     | akeed-restaurant-recommendation-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 71  | Public Hack  |    Prediction    |      $300 USD      | akeed-restaurant-recommendation-hackathon...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 72  | Public Hack  |  Classification  |      $300 USD      | covid-19-tweet-classification-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 73  |Public Compet |  Classification  |     $4 200 USD     | basic-needs-basic-rights-kenya-tech4mentalhealth...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 74  | Public Hack  |  Classification  |      $300 USD      | to-vaccinate-or-not-to-vaccinate-its-not-a-questio...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 75  | Public Hack  | Computer Vision  |      $300 USD      | spot-the-mask-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 76  | Public Hack  |    Prediction    |      $300 USD      | urban-air-pollution-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 77  | Public Hack  |    Prediction    |      $300 USD      | south-african-covid-19-vulnerability-map...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 78  |Public Compet |  Visualisation   |     $3 195 USD     | animal-insights-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 79  |Public Compet |     Forecast     |     $5 000 USD     | airqo-ugandan-air-quality-forecast-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 80  |Public Compet |     Forecast     |     $5 000 USD     | predict-the-global-spread-of-covid-19...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 81  |Public Compet | Computer Vision  |     $5 000 USD     | iclr-workshop-challenge-2-radiant-earth-computer-v...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 82  |Public Compet | Computer Vision  |     $5 000 USD     | iclr-workshop-challenge-1-cgiar-computer-vision-fo...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 83  |Public Compet |  Classification  | 2000 Zindi Points  | fowl-escapades...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 84  |Public Compet |     Forecast     |    $10 000 USD     | 2030-vision-flood-prediction-in-malawi...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 85  |Public Compet |  Visualisation   |     $5 000 USD     | 2030-vision-data-visualization-and-reporting-chall...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 86  |Public Compet |     Forecast     | 2000 Zindi Points  | sea-turtle-rescue-forecast-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 87  |Public Compet |    Prediction    |     $5 000 USD     | womxn-in-big-data-south-africa-female-headed-house...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 88  |Public Compet |  Classification  | 2000 Zindi Points  | tic-heap-cirta-particle-classification-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 89  |Public Compet |    Prediction    |     $5 500 USD     | uber-movement-sanral-cape-town-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 90  |Public Compet |    Prediction    | 2000 Zindi Points  | sbtic-xente-credit-scoring-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 91  |Public Compet |Reinforcement Lear|     Knowledge      | ibm-malaria-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 92  |Public Compet |    Prediction    |     $7 000 USD     | sendy-logistics-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 93  |Public Compet |     Forecast     |     $8 000 USD     | wazihub-soil-moisture-prediction-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 94  |Public Compet |  Visualisation   |     $3 000 USD     | ai-art... \n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 95  |Public Compet |  Classification  |     $4 500 USD     | xente-fraud-detection-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 96  |Public Compet |     Forecast     | 2000 Zindi Points  | mtoto-news-childline-kenya-call-volume-prediction-...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 97  |Public Compet |    Prediction    |Cash and prizes wort| mobile-money-and-financial-inclusion-in-tanzania-c...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 98  |Public Compet |  Classification  |    $11 000 USD     | farm-pin-crop-detection-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 99  |Public Compet |  Classification  |Cash and prizes wort| sea-turtle-rescue-error-detection-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 100 |Public Compet |    Prediction    |  500 Zindi Points  | busara-mental-health-prediction-challenge...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 101 |Public Compet |    Prediction    |    $12 000 USD     | traffic-jam-predicting-peoples-movement-into-nairo...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 102 |Public Compet |  Classification  |     $1 000 USD     | sustainable-development-goals-sdgs-text-classifica...\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "| 103 |Public Compet |    Prediction    |     $1 000 USD     | social-media-prediction-challenge...\n",
            "__________________________________________________________________________________________________________________________________\n",
            "\n",
            "\n",
            "Type the index of the challenge you want to select or 'q' to exit.\n",
            ">>7\n",
            "\n",
            "[  ] You choose the challenge : makerere-passion-fruit-disease-detection-challenge,\n",
            "\tCan you identify which passion fruit are diseased and which are healthy?.\n",
            "\n",
            "\n",
            "[  ] You are currently enrolled in : makerere-passion-fruit-disease-detection-challenge challenge,\n",
            "\tCan you identify which passion fruit are diseased and which are healthy?.\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'makerere-passion-fruit-disease-detection-challenge'"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dt7EG_OEEfAL"
      },
      "source": [
        "Download and unzip the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBK97UK8P3_O",
        "outputId": "13b06848-3ce8-480e-f134-acf8c7e4f53e"
      },
      "source": [
        "zindi_account.download_dataset(destination=\"dataset\")\n",
        "!unzip 'dataset/Train_Images.zip' -d ''\n",
        "!unzip 'dataset/Test_Images.zip' -d ''"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "dataset/PassionfruitStarterNotebook.ipynb: 100%|| 609k/609k [00:00<00:00, 12.7Mo/s]\n",
            "dataset/Test.csv: 100%|| 10.9k/10.9k [00:00<00:00, 2.35Mo/s]\n",
            "dataset/Sample_submission.csv: 100%|| 16.4k/16.4k [00:00<00:00, 3.99Mo/s]\n",
            "dataset/Train_Images.zip: 100%|| 293M/293M [00:08<00:00, 35.2Mo/s]\n",
            "dataset/Test_Images.zip: 100%|| 36.4M/36.4M [00:01<00:00, 35.2Mo/s]\n",
            "dataset/Train.csv: 100%|| 192k/192k [00:00<00:00, 6.29Mo/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  dataset/Train_Images.zip\n",
            "replace __MACOSX/._Train_Images? [y]es, [n]o, [A]ll, [N]one, [r]ename: Archive:  dataset/Test_Images.zip\n",
            "replace __MACOSX/._Test_Images? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6wYfPTIFYEF"
      },
      "source": [
        "Create a data frame with the columns we need"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHxUZbnHirqw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "0449387c-bbdf-4fe3-e486-f77214c75dae"
      },
      "source": [
        "train_dir = 'dataset/Train.csv'\n",
        "test_dir = 'dataset/Test.csv'\n",
        "train_df = pd.read_csv(train_dir)\n",
        "test_df = pd.read_csv(test_dir)\n",
        "train_df['xmax'] = train_df['xmin']+train_df['width']\n",
        "train_df['ymax'] = train_df['ymin']+train_df['height']\n",
        "classes_la = {\"fruit_brownspot\": 1, \"fruit_healthy\": 2, \"fruit_woodiness\":3}\n",
        "classes_la = {\"fruit_brownspot\": 1, \"fruit_healthy\": 2, \"fruit_woodiness\":3}\n",
        "train_df[\"class\"] = train_df[\"class\"].apply(lambda x: classes_la[x])\n",
        "df = train_df.copy()\n",
        "df.drop('width', inplace=True, axis=1)\n",
        "df.drop('height', inplace=True, axis=1)\n",
        "df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image_ID</th>\n",
              "      <th>class</th>\n",
              "      <th>xmin</th>\n",
              "      <th>ymin</th>\n",
              "      <th>xmax</th>\n",
              "      <th>ymax</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ID_007FAIEI</td>\n",
              "      <td>3</td>\n",
              "      <td>87.0</td>\n",
              "      <td>87.5</td>\n",
              "      <td>315.0</td>\n",
              "      <td>398.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ID_00G8K1V3</td>\n",
              "      <td>1</td>\n",
              "      <td>97.5</td>\n",
              "      <td>17.5</td>\n",
              "      <td>342.5</td>\n",
              "      <td>372.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ID_00WROUT9</td>\n",
              "      <td>1</td>\n",
              "      <td>156.5</td>\n",
              "      <td>209.5</td>\n",
              "      <td>404.5</td>\n",
              "      <td>512.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ID_00ZJEEK3</td>\n",
              "      <td>2</td>\n",
              "      <td>125.0</td>\n",
              "      <td>193.0</td>\n",
              "      <td>379.5</td>\n",
              "      <td>410.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ID_018UIENR</td>\n",
              "      <td>1</td>\n",
              "      <td>79.5</td>\n",
              "      <td>232.5</td>\n",
              "      <td>313.0</td>\n",
              "      <td>414.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Image_ID  class   xmin   ymin   xmax   ymax\n",
              "0  ID_007FAIEI      3   87.0   87.5  315.0  398.5\n",
              "1  ID_00G8K1V3      1   97.5   17.5  342.5  372.0\n",
              "2  ID_00WROUT9      1  156.5  209.5  404.5  512.0\n",
              "3  ID_00ZJEEK3      2  125.0  193.0  379.5  410.0\n",
              "4  ID_018UIENR      1   79.5  232.5  313.0  414.5"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQfAKxkeF0p5"
      },
      "source": [
        "Initialize and populate our lists of data, labels, bounding boxes and image paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzWeiUK3oe-k"
      },
      "source": [
        "data = [] # list of images (in array form)\n",
        "labels = [] # list of labels for each image\n",
        "bboxes = [] # list of bounding boxes for each image\n",
        "imagePaths = [] # list of paths for each image\n",
        "\n",
        "df = df.to_numpy()\n",
        "df = df.tolist()\n",
        "for row in df:\n",
        "  imageId, label, startX, startY, endX, endY = row\n",
        "  imagePath = 'Train_Images/'+imageId + '.jpg'\n",
        "\n",
        "  #load image\n",
        "  image = cv2.imread(imagePath)\n",
        "  (h,w) = image.shape[:2]\n",
        "\n",
        "  #scale bounding boxes\n",
        "  startX  = float(startX) / w\n",
        "  startY  = float(startY) / h\n",
        "  endX  = float(endX) / w\n",
        "  endY  = float(endY) / h\n",
        "\n",
        "  #load and preproess image\n",
        "  image = load_img(imagePath, target_size=(224,224))\n",
        "  image = img_to_array(image)\n",
        "\n",
        "  #update\n",
        "  data.append(image)\n",
        "  labels.append(label)\n",
        "  bboxes.append((startX, startY, endX, endY))\n",
        "  imagePaths.append(imagePath)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvLsdmcYHKeA"
      },
      "source": [
        "Some pre-processing - scale image data between 0 and 1 and perform binary one-hot encoding on labels. We also need to convert to numpy arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBDxRZFJrj8J"
      },
      "source": [
        "data = np.array(data, dtype='float32')/255.0\n",
        "labels = np.array(labels)\n",
        "bboxes = np.array(bboxes,dtype='float32')\n",
        "imagePaths = np.array(imagePaths)\n",
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(labels)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHd1S2A2I1u8"
      },
      "source": [
        "Perform training and testing split and define dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2_SAEfqsGXd"
      },
      "source": [
        "split = train_test_split(data, labels, bboxes, imagePaths,\n",
        "\ttest_size=0.2, random_state=20)\n",
        "\n",
        "(trainImages, testImages) = split[:2]\n",
        "(trainLabels, testLabels) = split[2:4]\n",
        "(trainBBoxes, testBBoxes) = split[4:6]\n",
        "(trainPaths, testPaths) = split[6:]\n",
        "\n",
        "# a dictionary for our target training outputs\n",
        "trainTargets = {\n",
        "\t\"class_label\": trainLabels,\n",
        "\t\"bounding_box\": trainBBoxes\n",
        "}\n",
        "# a second dictionary, this one for our target testing outputs\n",
        "testTargets = {\n",
        "\t\"class_label\": testLabels,\n",
        "\t\"bounding_box\": testBBoxes\n",
        "}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP1_BMc6JHZU"
      },
      "source": [
        "Define a function to build our model, given a set of hyper-parameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDunmlxcsYdQ"
      },
      "source": [
        "def get_model(hp):\n",
        "\t# Choose an optimal value for the number of units in the first hidden layer\n",
        "\thp_first_units = hp.Int('first_units', min_value=16, max_value=64, step=16)\n",
        "\t# Choose an optimal value for the number of units in the second hidden layer\n",
        "\thp_second_units = hp.Int('second_units', min_value=16, max_value=64, step=16)\n",
        "\t# Choose an optimal value from 0.01, 0.001, or 0.0001 for the learning rate\n",
        "\thp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "\t# Convolutional layers\n",
        "\trn50v2 = ResNet50V2(weights=\"imagenet\", include_top=False,\n",
        "\t\tinput_tensor=Input(shape=(224, 224, 3)))\n",
        "\trn50v2.trainable = False\n",
        "\tflatten = rn50v2.output\n",
        "\tflatten = Flatten()(flatten)\n",
        "\n",
        "\t# branch for output of bounding box coordinates\n",
        "\tbboxHead = Dense(hp_first_units, activation=\"relu\")(flatten)\n",
        "\tbboxHead = Dense(hp_second_units, activation=\"relu\")(bboxHead)\n",
        "\tbboxHead = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bboxHead)\n",
        "\n",
        "\t# branch for output of label\n",
        "\tsoftmaxHead = Dense(hp_first_units, activation=\"relu\")(flatten)\n",
        "\tsoftmaxHead = Dropout(0.5)(softmaxHead)\n",
        "\tsoftmaxHead = Dense(hp_second_units, activation=\"relu\")(softmaxHead)\n",
        "\tsoftmaxHead = Dropout(0.5)(softmaxHead)\n",
        "\tsoftmaxHead = Dense(len(lb.classes_), activation=\"softmax\", name=\"class_label\")(softmaxHead)\n",
        "\n",
        "\tmodel = Model(inputs=rn50v2.input, outputs=(bboxHead, softmaxHead))\n",
        "\tlosses = {\"class_label\": \"categorical_crossentropy\", \"bounding_box\": \"mean_squared_error\"}\n",
        "\tlossWeights = {\"class_label\": 1.0, \"bounding_box\": 1.0}\n",
        "\topt = Adam(learning_rate=hp_learning_rate)\n",
        "\tmodel.compile(loss=losses, optimizer=opt, metrics=[\"accuracy\"], loss_weights=lossWeights)\n",
        "\treturn model"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2R2Gzg9mYPr9"
      },
      "source": [
        "Tune the hyper-parameters using the hyperband tuner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHUSBRshY_Vs",
        "outputId": "e7c33df5-9ce5-433d-bfd9-387a564415b5"
      },
      "source": [
        "tuner = kt.Hyperband(get_model,\n",
        "                     objective='val_loss',\n",
        "                     max_epochs=10,\n",
        "                     factor=3,\n",
        "                     directory='hp_tuning',\n",
        "                     project_name='cos_711_assignment_3')\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "tuner.search(trainImages, trainTargets,\n",
        "\tvalidation_data=(testImages, testTargets),\n",
        "\tbatch_size=32,\n",
        "\tepochs=10,\n",
        "\tverbose=1,\n",
        "  callbacks=[stop_early])\n",
        "\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "tuner.results_summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results summary\n",
            "Results in hp_tuning/cos_711_assignment_3\n",
            "Showing 10 best trials\n",
            "Objective(name='val_loss', direction='min')\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "first_units: 48\n",
            "second_units: 32\n",
            "learning_rate: 0.0001\n",
            "tuner/epochs: 10\n",
            "tuner/initial_epoch: 4\n",
            "tuner/bracket: 1\n",
            "tuner/round: 1\n",
            "tuner/trial_id: bfca5869210f551642f946e42c40b6d6\n",
            "Score: 0.20852258801460266\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "first_units: 48\n",
            "second_units: 64\n",
            "learning_rate: 0.0001\n",
            "tuner/epochs: 10\n",
            "tuner/initial_epoch: 4\n",
            "tuner/bracket: 2\n",
            "tuner/round: 2\n",
            "tuner/trial_id: cbca625c76a5a482752f5fbb3b73642e\n",
            "Score: 0.2089443951845169\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "first_units: 48\n",
            "second_units: 16\n",
            "learning_rate: 0.0001\n",
            "tuner/epochs: 10\n",
            "tuner/initial_epoch: 4\n",
            "tuner/bracket: 2\n",
            "tuner/round: 2\n",
            "tuner/trial_id: 5fbde85174911115070d12a21990b790\n",
            "Score: 0.29474472999572754\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "first_units: 48\n",
            "second_units: 48\n",
            "learning_rate: 0.0001\n",
            "tuner/epochs: 2\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 2\n",
            "tuner/round: 0\n",
            "Score: 0.3611770570278168\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "first_units: 48\n",
            "second_units: 64\n",
            "learning_rate: 0.0001\n",
            "tuner/epochs: 2\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 2\n",
            "tuner/round: 0\n",
            "Score: 0.39165499806404114\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "first_units: 48\n",
            "second_units: 64\n",
            "learning_rate: 0.0001\n",
            "tuner/epochs: 4\n",
            "tuner/initial_epoch: 2\n",
            "tuner/bracket: 2\n",
            "tuner/round: 1\n",
            "tuner/trial_id: 2cf59c958e9b9c249efb7ce343f744c0\n",
            "Score: 0.41488781571388245\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "first_units: 48\n",
            "second_units: 16\n",
            "learning_rate: 0.0001\n",
            "tuner/epochs: 4\n",
            "tuner/initial_epoch: 2\n",
            "tuner/bracket: 2\n",
            "tuner/round: 1\n",
            "tuner/trial_id: 10afd591ca98eefc3794e45a640c9bfd\n",
            "Score: 0.4380328059196472\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "first_units: 64\n",
            "second_units: 16\n",
            "learning_rate: 0.001\n",
            "tuner/epochs: 4\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 1\n",
            "tuner/round: 0\n",
            "Score: 0.43831905722618103\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "first_units: 32\n",
            "second_units: 32\n",
            "learning_rate: 0.0001\n",
            "tuner/epochs: 2\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 2\n",
            "tuner/round: 0\n",
            "Score: 0.4563002586364746\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "first_units: 48\n",
            "second_units: 16\n",
            "learning_rate: 0.0001\n",
            "tuner/epochs: 2\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 2\n",
            "tuner/round: 0\n",
            "Score: 0.4714261293411255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tENESFSgJ9rb"
      },
      "source": [
        "Define function for best model now that we have the best hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGQ2cf7AKCVD"
      },
      "source": [
        "def best_model():\n",
        "\t# Choose an optimal value for the number of units in the first hidden layer\n",
        "\thp_first_units = 48\n",
        "\t# Choose an optimal value for the number of units in the second hidden layer\n",
        "\thp_second_units = 32\n",
        "\t# Choose an optimal value from 0.01, 0.001, or 0.0001 for the learning rate\n",
        "\thp_learning_rate = 1e-4\n",
        "\n",
        "\t# Convolutional layers\n",
        "\trn50v2 = ResNet50V2(weights=\"imagenet\", include_top=False,\n",
        "\t\tinput_tensor=Input(shape=(224, 224, 3)))\n",
        "\trn50v2.trainable = False\n",
        "\tflatten = rn50v2.output\n",
        "\tflatten = Flatten()(flatten)\n",
        "\n",
        "\t# branch for output of bounding box coordinates\n",
        "\tbboxHead = Dense(hp_first_units, activation=\"relu\")(flatten)\n",
        "\tbboxHead = Dense(hp_second_units, activation=\"relu\")(bboxHead)\n",
        "\tbboxHead = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bboxHead)\n",
        "\n",
        "\t# branch for output of label\n",
        "\tsoftmaxHead = Dense(hp_first_units, activation=\"relu\")(flatten)\n",
        "\tsoftmaxHead = Dropout(0.5)(softmaxHead)\n",
        "\tsoftmaxHead = Dense(hp_second_units, activation=\"relu\")(softmaxHead)\n",
        "\tsoftmaxHead = Dropout(0.5)(softmaxHead)\n",
        "\tsoftmaxHead = Dense(len(lb.classes_), activation=\"softmax\", name=\"class_label\")(softmaxHead)\n",
        "\n",
        "\tmodel = Model(inputs=rn50v2.input, outputs=(bboxHead, softmaxHead))\n",
        "\tlosses = {\"class_label\": \"categorical_crossentropy\", \"bounding_box\": \"mean_squared_error\"}\n",
        "\tlossWeights = {\"class_label\": 1.0, \"bounding_box\": 1.0}\n",
        "\topt = Adam(learning_rate=hp_learning_rate)\n",
        "\tmodel.compile(loss=losses, optimizer=opt, metrics=[\"accuracy\"], loss_weights=lossWeights)\n",
        "\treturn model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBiY_YkWT1cZ"
      },
      "source": [
        "Run tests using the best hyperparamters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ye1yPP5ZT5Kj",
        "outputId": "9805d88d-56fb-41eb-8531-8100a2af0a23"
      },
      "source": [
        "histories = []\n",
        "for i in range(10):\n",
        "  tf.keras.backend.clear_session()\n",
        "  model = best_model()\n",
        "  history = model.fit(trainImages, trainTargets,validation_data=(testImages, testTargets),batch_size=32,epochs=10,verbose=1)\n",
        "  histories.append(history)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "98/98 [==============================] - 61s 278ms/step - loss: 1.0303 - bounding_box_loss: 0.0354 - class_label_loss: 0.9949 - bounding_box_accuracy: 0.6021 - class_label_accuracy: 0.5755 - val_loss: 0.5086 - val_bounding_box_loss: 0.0269 - val_class_label_loss: 0.4817 - val_bounding_box_accuracy: 0.6419 - val_class_label_accuracy: 0.8555\n",
            "Epoch 2/10\n",
            "98/98 [==============================] - 24s 242ms/step - loss: 0.7384 - bounding_box_loss: 0.0233 - class_label_loss: 0.7150 - bounding_box_accuracy: 0.7017 - class_label_accuracy: 0.6917 - val_loss: 0.4115 - val_bounding_box_loss: 0.0282 - val_class_label_loss: 0.3832 - val_bounding_box_accuracy: 0.6586 - val_class_label_accuracy: 0.9066\n",
            "Epoch 3/10\n",
            "98/98 [==============================] - 24s 244ms/step - loss: 0.6425 - bounding_box_loss: 0.0212 - class_label_loss: 0.6213 - bounding_box_accuracy: 0.7372 - class_label_accuracy: 0.7340 - val_loss: 0.3055 - val_bounding_box_loss: 0.0284 - val_class_label_loss: 0.2771 - val_bounding_box_accuracy: 0.6982 - val_class_label_accuracy: 0.9297\n",
            "Epoch 4/10\n",
            "98/98 [==============================] - 24s 244ms/step - loss: 0.6004 - bounding_box_loss: 0.0198 - class_label_loss: 0.5807 - bounding_box_accuracy: 0.7484 - class_label_accuracy: 0.7401 - val_loss: 0.2491 - val_bounding_box_loss: 0.0293 - val_class_label_loss: 0.2199 - val_bounding_box_accuracy: 0.6803 - val_class_label_accuracy: 0.9425\n",
            "Epoch 5/10\n",
            "98/98 [==============================] - 24s 246ms/step - loss: 0.5208 - bounding_box_loss: 0.0187 - class_label_loss: 0.5021 - bounding_box_accuracy: 0.7711 - class_label_accuracy: 0.7705 - val_loss: 0.2192 - val_bounding_box_loss: 0.0297 - val_class_label_loss: 0.1895 - val_bounding_box_accuracy: 0.6957 - val_class_label_accuracy: 0.9463\n",
            "Epoch 6/10\n",
            "98/98 [==============================] - 24s 245ms/step - loss: 0.5022 - bounding_box_loss: 0.0182 - class_label_loss: 0.4840 - bounding_box_accuracy: 0.7778 - class_label_accuracy: 0.7673 - val_loss: 0.1798 - val_bounding_box_loss: 0.0303 - val_class_label_loss: 0.1495 - val_bounding_box_accuracy: 0.6841 - val_class_label_accuracy: 0.9604\n",
            "Epoch 7/10\n",
            "98/98 [==============================] - 24s 245ms/step - loss: 0.4642 - bounding_box_loss: 0.0175 - class_label_loss: 0.4467 - bounding_box_accuracy: 0.7900 - class_label_accuracy: 0.7916 - val_loss: 0.1834 - val_bounding_box_loss: 0.0311 - val_class_label_loss: 0.1523 - val_bounding_box_accuracy: 0.6752 - val_class_label_accuracy: 0.9527\n",
            "Epoch 8/10\n",
            "98/98 [==============================] - 24s 244ms/step - loss: 0.4105 - bounding_box_loss: 0.0174 - class_label_loss: 0.3932 - bounding_box_accuracy: 0.7891 - class_label_accuracy: 0.8140 - val_loss: 0.1755 - val_bounding_box_loss: 0.0309 - val_class_label_loss: 0.1445 - val_bounding_box_accuracy: 0.6957 - val_class_label_accuracy: 0.9540\n",
            "Epoch 9/10\n",
            "98/98 [==============================] - 24s 244ms/step - loss: 0.3715 - bounding_box_loss: 0.0170 - class_label_loss: 0.3545 - bounding_box_accuracy: 0.7913 - class_label_accuracy: 0.8268 - val_loss: 0.1642 - val_bounding_box_loss: 0.0309 - val_class_label_loss: 0.1333 - val_bounding_box_accuracy: 0.6893 - val_class_label_accuracy: 0.9616\n",
            "Epoch 10/10\n",
            "98/98 [==============================] - 24s 245ms/step - loss: 0.3544 - bounding_box_loss: 0.0168 - class_label_loss: 0.3376 - bounding_box_accuracy: 0.7843 - class_label_accuracy: 0.8281 - val_loss: 0.1537 - val_bounding_box_loss: 0.0327 - val_class_label_loss: 0.1210 - val_bounding_box_accuracy: 0.6777 - val_class_label_accuracy: 0.9629\n",
            "Epoch 1/10\n",
            "98/98 [==============================] - 30s 259ms/step - loss: 1.1074 - bounding_box_loss: 0.0422 - class_label_loss: 1.0652 - bounding_box_accuracy: 0.6572 - class_label_accuracy: 0.4914 - val_loss: 0.5556 - val_bounding_box_loss: 0.0326 - val_class_label_loss: 0.5230 - val_bounding_box_accuracy: 0.7059 - val_class_label_accuracy: 0.8517\n",
            "Epoch 2/10\n",
            "98/98 [==============================] - 24s 245ms/step - loss: 0.8246 - bounding_box_loss: 0.0274 - class_label_loss: 0.7972 - bounding_box_accuracy: 0.7151 - class_label_accuracy: 0.6488 - val_loss: 0.4390 - val_bounding_box_loss: 0.0308 - val_class_label_loss: 0.4081 - val_bounding_box_accuracy: 0.6816 - val_class_label_accuracy: 0.8926\n",
            "Epoch 3/10\n",
            "98/98 [==============================] - 24s 245ms/step - loss: 0.6644 - bounding_box_loss: 0.0237 - class_label_loss: 0.6407 - bounding_box_accuracy: 0.7378 - class_label_accuracy: 0.7065 - val_loss: 0.3646 - val_bounding_box_loss: 0.0334 - val_class_label_loss: 0.3312 - val_bounding_box_accuracy: 0.6675 - val_class_label_accuracy: 0.9220\n",
            "Epoch 4/10\n",
            "98/98 [==============================] - 24s 246ms/step - loss: 0.5982 - bounding_box_loss: 0.0208 - class_label_loss: 0.5774 - bounding_box_accuracy: 0.7612 - class_label_accuracy: 0.7401 - val_loss: 0.3164 - val_bounding_box_loss: 0.0338 - val_class_label_loss: 0.2826 - val_bounding_box_accuracy: 0.7020 - val_class_label_accuracy: 0.9284\n",
            "Epoch 5/10\n",
            "98/98 [==============================] - 24s 245ms/step - loss: 0.5450 - bounding_box_loss: 0.0196 - class_label_loss: 0.5254 - bounding_box_accuracy: 0.7647 - class_label_accuracy: 0.7698 - val_loss: 0.2463 - val_bounding_box_loss: 0.0341 - val_class_label_loss: 0.2122 - val_bounding_box_accuracy: 0.6969 - val_class_label_accuracy: 0.9514\n",
            "Epoch 6/10\n",
            "98/98 [==============================] - 24s 246ms/step - loss: 0.5043 - bounding_box_loss: 0.0185 - class_label_loss: 0.4858 - bounding_box_accuracy: 0.7862 - class_label_accuracy: 0.7961 - val_loss: 0.2155 - val_bounding_box_loss: 0.0352 - val_class_label_loss: 0.1803 - val_bounding_box_accuracy: 0.6777 - val_class_label_accuracy: 0.9540\n",
            "Epoch 7/10\n",
            "98/98 [==============================] - 24s 245ms/step - loss: 0.4661 - bounding_box_loss: 0.0178 - class_label_loss: 0.4483 - bounding_box_accuracy: 0.7807 - class_label_accuracy: 0.8147 - val_loss: 0.1876 - val_bounding_box_loss: 0.0355 - val_class_label_loss: 0.1521 - val_bounding_box_accuracy: 0.6752 - val_class_label_accuracy: 0.9604\n",
            "Epoch 8/10\n",
            "98/98 [==============================] - 24s 245ms/step - loss: 0.4189 - bounding_box_loss: 0.0173 - class_label_loss: 0.4016 - bounding_box_accuracy: 0.7980 - class_label_accuracy: 0.8326 - val_loss: 0.1706 - val_bounding_box_loss: 0.0353 - val_class_label_loss: 0.1353 - val_bounding_box_accuracy: 0.6701 - val_class_label_accuracy: 0.9591\n",
            "Epoch 9/10\n",
            "98/98 [==============================] - 24s 245ms/step - loss: 0.3852 - bounding_box_loss: 0.0172 - class_label_loss: 0.3681 - bounding_box_accuracy: 0.7932 - class_label_accuracy: 0.8431 - val_loss: 0.1631 - val_bounding_box_loss: 0.0361 - val_class_label_loss: 0.1271 - val_bounding_box_accuracy: 0.6739 - val_class_label_accuracy: 0.9540\n",
            "Epoch 10/10\n",
            "98/98 [==============================] - 24s 245ms/step - loss: 0.3479 - bounding_box_loss: 0.0168 - class_label_loss: 0.3310 - bounding_box_accuracy: 0.7990 - class_label_accuracy: 0.8598 - val_loss: 0.1396 - val_bounding_box_loss: 0.0359 - val_class_label_loss: 0.1037 - val_bounding_box_accuracy: 0.6995 - val_class_label_accuracy: 0.9591\n",
            "Epoch 1/10\n",
            "98/98 [==============================] - 31s 270ms/step - loss: 1.0118 - bounding_box_loss: 0.0442 - class_label_loss: 0.9675 - bounding_box_accuracy: 0.6402 - class_label_accuracy: 0.5535 - val_loss: 0.6188 - val_bounding_box_loss: 0.0281 - val_class_label_loss: 0.5906 - val_bounding_box_accuracy: 0.6573 - val_class_label_accuracy: 0.7084\n",
            "Epoch 2/10\n",
            "98/98 [==============================] - 24s 245ms/step - loss: 0.7608 - bounding_box_loss: 0.0252 - class_label_loss: 0.7357 - bounding_box_accuracy: 0.7353 - class_label_accuracy: 0.6287 - val_loss: 0.5198 - val_bounding_box_loss: 0.0280 - val_class_label_loss: 0.4919 - val_bounding_box_accuracy: 0.6624 - val_class_label_accuracy: 0.7174\n",
            "Epoch 3/10\n",
            "98/98 [==============================] - 24s 250ms/step - loss: 0.7326 - bounding_box_loss: 0.0214 - class_label_loss: 0.7112 - bounding_box_accuracy: 0.7564 - class_label_accuracy: 0.6482 - val_loss: 0.5049 - val_bounding_box_loss: 0.0282 - val_class_label_loss: 0.4767 - val_bounding_box_accuracy: 0.6675 - val_class_label_accuracy: 0.7225\n",
            "Epoch 4/10\n",
            "98/98 [==============================] - 24s 245ms/step - loss: 0.6659 - bounding_box_loss: 0.0197 - class_label_loss: 0.6462 - bounding_box_accuracy: 0.7702 - class_label_accuracy: 0.6783 - val_loss: 0.4640 - val_bounding_box_loss: 0.0291 - val_class_label_loss: 0.4349 - val_bounding_box_accuracy: 0.6816 - val_class_label_accuracy: 0.7199\n",
            "Epoch 5/10\n",
            "98/98 [==============================] - 24s 245ms/step - loss: 0.6080 - bounding_box_loss: 0.0187 - class_label_loss: 0.5893 - bounding_box_accuracy: 0.7814 - class_label_accuracy: 0.7100 - val_loss: 0.4608 - val_bounding_box_loss: 0.0298 - val_class_label_loss: 0.4311 - val_bounding_box_accuracy: 0.7097 - val_class_label_accuracy: 0.7187\n",
            "Epoch 6/10\n",
            "98/98 [==============================] - 24s 246ms/step - loss: 0.5935 - bounding_box_loss: 0.0183 - class_label_loss: 0.5753 - bounding_box_accuracy: 0.7814 - class_label_accuracy: 0.6997 - val_loss: 0.4446 - val_bounding_box_loss: 0.0303 - val_class_label_loss: 0.4142 - val_bounding_box_accuracy: 0.6637 - val_class_label_accuracy: 0.7263\n",
            "Epoch 7/10\n",
            "98/98 [==============================] - 24s 245ms/step - loss: 0.5982 - bounding_box_loss: 0.0176 - class_label_loss: 0.5806 - bounding_box_accuracy: 0.7929 - class_label_accuracy: 0.7100 - val_loss: 0.4144 - val_bounding_box_loss: 0.0303 - val_class_label_loss: 0.3841 - val_bounding_box_accuracy: 0.6662 - val_class_label_accuracy: 0.7276\n",
            "Epoch 8/10\n",
            "98/98 [==============================] - 24s 246ms/step - loss: 0.5675 - bounding_box_loss: 0.0169 - class_label_loss: 0.5505 - bounding_box_accuracy: 0.8019 - class_label_accuracy: 0.7238 - val_loss: 0.4100 - val_bounding_box_loss: 0.0317 - val_class_label_loss: 0.3783 - val_bounding_box_accuracy: 0.6816 - val_class_label_accuracy: 0.9361\n",
            "Epoch 9/10\n",
            "98/98 [==============================] - 24s 246ms/step - loss: 0.5444 - bounding_box_loss: 0.0164 - class_label_loss: 0.5280 - bounding_box_accuracy: 0.8003 - class_label_accuracy: 0.7558 - val_loss: 0.4071 - val_bounding_box_loss: 0.0324 - val_class_label_loss: 0.3747 - val_bounding_box_accuracy: 0.6816 - val_class_label_accuracy: 0.9412\n",
            "Epoch 10/10\n",
            "98/98 [==============================] - 24s 245ms/step - loss: 0.5326 - bounding_box_loss: 0.0164 - class_label_loss: 0.5162 - bounding_box_accuracy: 0.8127 - class_label_accuracy: 0.7638 - val_loss: 0.3949 - val_bounding_box_loss: 0.0322 - val_class_label_loss: 0.3627 - val_bounding_box_accuracy: 0.6880 - val_class_label_accuracy: 0.9309\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4u36gLd4Q4U"
      },
      "source": [
        "Tabulate the results based on accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6hNlEfU4T5N",
        "outputId": "6042e843-cbdb-4458-b988-5ed768654337"
      },
      "source": [
        "lblTrainingAcc = statistics.mean(history.history['class_label_accuracy'][-1] for history in histories)\n",
        "lblTrainingStd = statistics.stdev(history.history['class_label_accuracy'][-1] for history in histories)\n",
        "lblValidationAcc = statistics.mean(history.history['val_class_label_accuracy'][-1] for history in histories)\n",
        "lblValidationStd = statistics.stdev(history.history['val_class_label_accuracy'][-1] for history in histories)\n",
        "\n",
        "bbTrainingAcc = statistics.mean(history.history['bounding_box_accuracy'][-1] for history in histories)\n",
        "bbTrainingStd = statistics.stdev(history.history['bounding_box_accuracy'][-1] for history in histories)\n",
        "bbValidationAcc = statistics.mean(history.history['val_bounding_box_accuracy'][-1] for history in histories)\n",
        "bbValidationStd = statistics.stdev(history.history['val_bounding_box_accuracy'][-1] for history in histories)\n",
        "\n",
        "table = [[\"Class Label\",lblTrainingAcc,lblTrainingStd,lblValidationAcc,lblValidationStd],\n",
        "         [\"Bounding Box\",bbTrainingAcc,bbTrainingStd,bbValidationAcc,bbValidationStd]]\n",
        "\n",
        "print(tabulate(table, headers=[\"\",\"Training\\nAccuracy\", \"Training\\n\", \"Validation\\nAccuracy\", \"Validation\\n\"], tablefmt=\"fancy_grid\"))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "もももも\n",
            "                 Training    Training    Validation    Validation \n",
            "                 Accuracy                 Accuracy              \n",
            "\n",
            " Class Label     0.817222   0.0489317      0.95098      0.0174557 \n",
            "尖尖尖尖\n",
            " Bounding Box    0.798656   0.0142473      0.688406     0.0108759 \n",
            "рррр\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MDoMW1GWYJB"
      },
      "source": [
        "What about loss? How did loss progress during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "x0JL6DwFWY0g",
        "outputId": "45731e97-6ff5-4880-c0e0-85b1f7049836"
      },
      "source": [
        "lblHistoriesList = list(map(lambda x: x.history['class_label_loss'], histories))\n",
        "avgLblAccProgress = np.average(lblHistoriesList, axis=0)\n",
        "lblValHistoriesList = list(map(lambda x: x.history['val_class_label_loss'], histories))\n",
        "avgLblValAccProgress = np.average(lblValHistoriesList, axis=0)\n",
        "bbHistoriesList = list(map(lambda x: x.history['bounding_box_loss'], histories))\n",
        "avgBbAccProgress = np.average(bbHistoriesList, axis=0)\n",
        "bbValHistoriesList = list(map(lambda x: x.history['val_bounding_box_loss'], histories))\n",
        "avgBbValAccProgress = np.average(bbValHistoriesList, axis=0)\n",
        "\n",
        "plt.plot(avgLblAccProgress, label='Class Label Loss')\n",
        "plt.plot(avgLblValAccProgress, label='Class Label Val. Loss')\n",
        "plt.plot(avgBbAccProgress, label='Bounding Box Loss')\n",
        "plt.plot(avgBbValAccProgress, label='Bounding Box Val. Loss')\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrIA87aYYSts"
      },
      "source": [
        "Create a submission for testing on Zindi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upSGBSAdXu2m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb027f27-7803-45ef-f20e-a9daa386685d"
      },
      "source": [
        "model = best_model()\n",
        "history = model.fit(trainImages, trainTargets,validation_data=(testImages, testTargets),batch_size=32,epochs=10,verbose=1)\n",
        "test_images_dir = 'Test_Images/'\n",
        "test_df.head()\n",
        "testImageIds = test_df['Image_ID'].to_numpy().tolist()\n",
        "testImageIds\n",
        "ids = []\n",
        "label = []\n",
        "xmin = []\n",
        "confidence = []\n",
        "ymin =[]\n",
        "xmax= []\n",
        "ymax=[]\n",
        "count = 0\n",
        "for tid in testImageIds:\n",
        "  imagePath = test_images_dir+tid+'.jpg'\n",
        "  image = load_img(imagePath, target_size=(224,224))\n",
        "  image = img_to_array(image)\n",
        "  image = np.array(image, dtype='float32')/255.0\n",
        "  image = np.expand_dims(image,axis=0)\n",
        "  prediction = model.predict(image)\n",
        "  startX,startY,endX,endY = prediction[0][0]\n",
        "  image = cv2.imread(imagePath)\n",
        "  (h,w) = image.shape[:2]\n",
        "  xmin.append(startX*w)\n",
        "  ymin.append(startY*h)\n",
        "  xmax.append(endX*w)\n",
        "  ymax.append(endY*h)\n",
        "  ids.append(tid)\n",
        "  ce = np.amax(prediction[1][0])\n",
        "  confidence.append(ce)\n",
        "  idx = np.argmax(prediction[1][0])\n",
        "  classes = [\"fruit_brownspot\",\"fruit_healthy\",\"fruit_woodiness\"]\n",
        "  label.append(classes[idx])\n",
        "  count += 1\n",
        "#data transform"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "98/98 [==============================] - 29s 257ms/step - loss: 1.0802 - bounding_box_loss: 0.0395 - class_label_loss: 1.0407 - bounding_box_accuracy: 0.6354 - class_label_accuracy: 0.5637 - val_loss: 0.6400 - val_bounding_box_loss: 0.0321 - val_class_label_loss: 0.6079 - val_bounding_box_accuracy: 0.6726 - val_class_label_accuracy: 0.7852\n",
            "Epoch 2/10\n",
            "98/98 [==============================] - 24s 244ms/step - loss: 0.7917 - bounding_box_loss: 0.0259 - class_label_loss: 0.7658 - bounding_box_accuracy: 0.7013 - class_label_accuracy: 0.6975 - val_loss: 0.5514 - val_bounding_box_loss: 0.0291 - val_class_label_loss: 0.5224 - val_bounding_box_accuracy: 0.6445 - val_class_label_accuracy: 0.8414\n",
            "Epoch 3/10\n",
            "98/98 [==============================] - 24s 245ms/step - loss: 0.7418 - bounding_box_loss: 0.0225 - class_label_loss: 0.7193 - bounding_box_accuracy: 0.7286 - class_label_accuracy: 0.7561 - val_loss: 0.4898 - val_bounding_box_loss: 0.0303 - val_class_label_loss: 0.4595 - val_bounding_box_accuracy: 0.6586 - val_class_label_accuracy: 0.9015\n",
            "Epoch 4/10\n",
            "98/98 [==============================] - 24s 245ms/step - loss: 0.6674 - bounding_box_loss: 0.0207 - class_label_loss: 0.6467 - bounding_box_accuracy: 0.7513 - class_label_accuracy: 0.7932 - val_loss: 0.4840 - val_bounding_box_loss: 0.0306 - val_class_label_loss: 0.4534 - val_bounding_box_accuracy: 0.6496 - val_class_label_accuracy: 0.9054\n",
            "Epoch 5/10\n",
            "98/98 [==============================] - 24s 245ms/step - loss: 0.6139 - bounding_box_loss: 0.0195 - class_label_loss: 0.5944 - bounding_box_accuracy: 0.7650 - class_label_accuracy: 0.8268 - val_loss: 0.4430 - val_bounding_box_loss: 0.0312 - val_class_label_loss: 0.4118 - val_bounding_box_accuracy: 0.6765 - val_class_label_accuracy: 0.9246\n",
            "Epoch 6/10\n",
            "98/98 [==============================] - 24s 245ms/step - loss: 0.5560 - bounding_box_loss: 0.0191 - class_label_loss: 0.5369 - bounding_box_accuracy: 0.7804 - class_label_accuracy: 0.8412 - val_loss: 0.4216 - val_bounding_box_loss: 0.0315 - val_class_label_loss: 0.3901 - val_bounding_box_accuracy: 0.6854 - val_class_label_accuracy: 0.9246\n",
            "Epoch 7/10\n",
            "98/98 [==============================] - 24s 246ms/step - loss: 0.5043 - bounding_box_loss: 0.0184 - class_label_loss: 0.4859 - bounding_box_accuracy: 0.7871 - class_label_accuracy: 0.8662 - val_loss: 0.3896 - val_bounding_box_loss: 0.0321 - val_class_label_loss: 0.3574 - val_bounding_box_accuracy: 0.6739 - val_class_label_accuracy: 0.9297\n",
            "Epoch 8/10\n",
            "98/98 [==============================] - 24s 245ms/step - loss: 0.5012 - bounding_box_loss: 0.0179 - class_label_loss: 0.4833 - bounding_box_accuracy: 0.7859 - class_label_accuracy: 0.8726 - val_loss: 0.3762 - val_bounding_box_loss: 0.0320 - val_class_label_loss: 0.3443 - val_bounding_box_accuracy: 0.6777 - val_class_label_accuracy: 0.9309\n",
            "Epoch 9/10\n",
            "98/98 [==============================] - 24s 245ms/step - loss: 0.4476 - bounding_box_loss: 0.0178 - class_label_loss: 0.4298 - bounding_box_accuracy: 0.7942 - class_label_accuracy: 0.8870 - val_loss: 0.3739 - val_bounding_box_loss: 0.0325 - val_class_label_loss: 0.3413 - val_bounding_box_accuracy: 0.6944 - val_class_label_accuracy: 0.9348\n",
            "Epoch 10/10\n",
            "98/98 [==============================] - 24s 245ms/step - loss: 0.4500 - bounding_box_loss: 0.0172 - class_label_loss: 0.4328 - bounding_box_accuracy: 0.8047 - class_label_accuracy: 0.8902 - val_loss: 0.3513 - val_bounding_box_loss: 0.0328 - val_class_label_loss: 0.3186 - val_bounding_box_accuracy: 0.6995 - val_class_label_accuracy: 0.9476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_NEFiwzcFkQ"
      },
      "source": [
        "solution = pd.DataFrame({'Image_ID':ids})\n",
        "solution['class'] = label\n",
        "solution['confidence'] = confidence\n",
        "solution['ymin'] = ymin\n",
        "solution['xmin'] = xmin\n",
        "solution['ymax'] = ymax\n",
        "solution['xmax'] = xmax\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4A9WXEK9j1bi"
      },
      "source": [
        "submission = solution.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KP35uTtvj4sj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "73edfe20-dda3-4acd-c210-0edb3a8983ff"
      },
      "source": [
        "submission.rename({0:'Image_ID'},axis='index')\n",
        "submission.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image_ID</th>\n",
              "      <th>class</th>\n",
              "      <th>confidence</th>\n",
              "      <th>ymin</th>\n",
              "      <th>xmin</th>\n",
              "      <th>ymax</th>\n",
              "      <th>xmax</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ID_IUJJG62B</td>\n",
              "      <td>fruit_healthy</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>279.840240</td>\n",
              "      <td>289.366791</td>\n",
              "      <td>426.820618</td>\n",
              "      <td>460.809418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ID_ZPNDRD4T</td>\n",
              "      <td>fruit_healthy</td>\n",
              "      <td>0.999998</td>\n",
              "      <td>263.239075</td>\n",
              "      <td>125.332031</td>\n",
              "      <td>404.609070</td>\n",
              "      <td>268.275757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ID_AHFYB64P</td>\n",
              "      <td>fruit_woodiness</td>\n",
              "      <td>0.999990</td>\n",
              "      <td>227.531403</td>\n",
              "      <td>217.595474</td>\n",
              "      <td>434.598969</td>\n",
              "      <td>378.175995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ID_L8JZLNTF</td>\n",
              "      <td>fruit_healthy</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>182.103851</td>\n",
              "      <td>118.848465</td>\n",
              "      <td>410.043030</td>\n",
              "      <td>349.787415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ID_IFMUXGPL</td>\n",
              "      <td>fruit_woodiness</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>157.412552</td>\n",
              "      <td>248.510040</td>\n",
              "      <td>343.166901</td>\n",
              "      <td>394.465088</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Image_ID            class  confidence  ...        xmin        ymax        xmax\n",
              "0  ID_IUJJG62B    fruit_healthy    1.000000  ...  289.366791  426.820618  460.809418\n",
              "1  ID_ZPNDRD4T    fruit_healthy    0.999998  ...  125.332031  404.609070  268.275757\n",
              "2  ID_AHFYB64P  fruit_woodiness    0.999990  ...  217.595474  434.598969  378.175995\n",
              "3  ID_L8JZLNTF    fruit_healthy    1.000000  ...  118.848465  410.043030  349.787415\n",
              "4  ID_IFMUXGPL  fruit_woodiness    1.000000  ...  248.510040  343.166901  394.465088\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zO7-yyxlXe8"
      },
      "source": [
        "submission.to_csv('submission.csv', index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}